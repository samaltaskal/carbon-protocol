# Carbon Protocol Tools

This directory contains **offline/build-time utilities** for the Carbon Protocol SDK. These tools are NOT part of the runtime SDK - they are used for generating and maintaining the skill dictionaries.

## Directory Structure

```
tools/
├── README.md                    # This file
├── build_dictionary.py          # Generate YAML dictionaries from various sources
├── convert_mined_to_patterns.py # Convert miner output to YAML patterns
└── miner/                       # Pattern discovery pipeline
    ├── __init__.py
    └── discovery.py             # N-gram extraction + DBSCAN clustering
```

## Tools Overview

### 1. build_dictionary.py

Automatically generates YAML dictionary files from various sources:
- Python/SQL reserved keywords
- VS Code snippets from GitHub
- Cloud CLI commands (AWS, Azure, GCP)

**Usage:**
```bash
python tools/build_dictionary.py --all                    # Build all dictionaries
python tools/build_dictionary.py --python                 # Python keywords only
python tools/build_dictionary.py --sql                    # SQL keywords only
python tools/build_dictionary.py --kubernetes             # Kubernetes snippets
python tools/build_dictionary.py --aws                    # AWS CLI commands
```

**Output:** `src/data/*.yaml` files

### 2. convert_mined_to_patterns.py

Converts mined cluster output to Carbon Protocol YAML patterns.

**Usage:**
```bash
python tools/convert_mined_to_patterns.py --input clusters.json --output src/data/mined_patterns.yaml
```

### 3. miner/ (Pattern Discovery Pipeline)

Analyzes large log volumes to discover frequent N-Grams and cluster them into semantic intents.

**Process:**
1. **Statistical**: CountVectorizer (N-Grams 3-6) → Top K phrases
2. **Semantic**: SentenceTransformer → Vector embeddings
3. **Clustering**: DBSCAN → Intent groups
4. **Synthesis**: JSON output candidate rules

**Dependencies:**
```bash
pip install scikit-learn sentence-transformers datasets
```

**Usage:**
```bash
# From a text file
python -m tools.miner.discovery --input logs.txt --output clusters.json

# From HuggingFace dataset
python -m tools.miner.discovery --hf-dataset "HuggingFaceFW/fineweb" --output clusters.json
```

## Generated Files

These tools have generated the following `src/data/` files:

| File | Generated By | Source |
|------|--------------|--------|
| `lang_python.yaml` | build_dictionary.py | Python keywords |
| `lang_sql.yaml` | build_dictionary.py | SQL keywords |
| `cloud_aws.yaml` | build_dictionary.py | AWS CLI |
| `cloud_azure.yaml` | build_dictionary.py | Azure CLI |
| `cloud_gcp.yaml` | build_dictionary.py | GCP CLI |
| `tools_kubernetes.yaml` | build_dictionary.py | K8s snippets |
| `mined_fineweb.yaml` | convert_mined_to_patterns.py | FineWeb dataset |
| `mined_openhermes.yaml` | convert_mined_to_patterns.py | OpenHermes dataset |

## When to Use These Tools

- **Adding new domains**: Run `build_dictionary.py` with appropriate flags
- **Discovering patterns from logs**: Run the miner pipeline, then convert
- **Updating existing dictionaries**: Re-run the relevant build command

These tools are separate from the runtime SDK to keep the core library lightweight and free of heavy ML dependencies (sklearn, sentence-transformers).
